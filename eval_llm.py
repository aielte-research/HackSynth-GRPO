import argparse
import os
import pandas as pd
import datetime
import time
from openai import OpenAI
import openai
import re
import dotenv
dotenv.load_dotenv()


def make_conversation(data, include_hint=False):
    if include_hint:
        user_content = data["question"] + "\n" + data["hint"]
    else:
        user_content = data["question"]

    user_content = "Solve the following CTF challenge and find the flag. Please reason step by step, and put your final answer within \\boxed{}.\n" + user_content

    return {
        "prompt": [
            {"role": "user", "content": user_content},
        ],
        "solution": data["solution"]
    }


def parse_flag_from_text(text):
    flag = re.findall(r'(?:\\boxed\{)?(flag\{.*?\})(?:\})?', text)
    if not flag:
        return None
    return flag[-1]


def parse_args():
    parser = argparse.ArgumentParser(
        description="Evaluate an LLM on a procedural dataset with Pass@N and Majority@N.",
    )

    # Dataset
    parser.add_argument(
        "--csv_path",
        type=str,
        required=True,
        help="Path to the CSV dataset containing columns `story` (question) and `flag` (solution).",
    )
    parser.add_argument(
        "--include_hint",
        action="store_true",
        help="If set, concatenate the hint after the question.",
    )

    # Backend / model
    parser.add_argument(
        "--llm_model_id",
        type=str,
        required=True,
        help="Model identifier (e.g. `gpt-4o` for OpenAI or a checkpoint path for vLLM).",
    )
    parser.add_argument(
        "--backend",
        type=str,
        required=True,
        choices=["vllm", "openai", "groqcloud"],
        help="Backend to use: 'vllm' or 'openai'.",
    )

    # Generation hyperâ€‘parameters
    parser.add_argument("--n_samples", type=int, default=8, help="Generations per query.")
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=4096,
        help="Maximum tokens per generation.",
    )
    parser.add_argument("--temperature", type=float, default=0.6, help="Sampling temperature.")
    parser.add_argument("--top_p", type=float, default=0.9, help="Nucleus sampling p.")

    return parser.parse_args()


def create_llm_pipeline(backend):
    if backend == "vllm":
        return OpenAI(
            api_key="sk-fake",  # any string is fine
            base_url="http://localhost:8000/v1"
        )
    elif backend == "groqcloud":
        print("API", os.environ.get("GROQ_API_KEY"))
        return OpenAI(
            api_key=os.environ.get("GROQ_API_KEY"),
            base_url="https://api.groq.com/openai/v1"
        )
    elif backend == "openai":
        return OpenAI()

    else:
        raise ValueError(f"Unknown backend: {backend}")


def generate_text(backend, model, llm_pipeline, messages, temperature, top_p, max_new_tokens, n):
    if backend == "openai" and model.startswith("o"):
        response = llm_pipeline.chat.completions.create(
            model=model,
            messages=messages,
            max_completion_tokens=max_new_tokens,
            n=n,
        )
    elif backend == "groqcloud":
        all_choices = []
        for _ in range(n):
            resp = llm_pipeline.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_new_tokens,
                n=1,  # force single completion
            )
            all_choices.extend(resp.choices)
            # return an object that mimics the normal API response
            class DummyResp:
                pass
            out = DummyResp()
            out.choices = all_choices
    else:
        response = llm_pipeline.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_new_tokens,
            n=n,
        )
    return response


def main():
    args = parse_args()

    overall_start = time.time()

    print(f"Loading procedural dataset from {args.csv_path}...")
    df = pd.read_csv(args.csv_path)
    df = df.rename(columns={"story": "question", "flag": "solution"})
    dataset = df[["question", "solution", "hint"]].to_dict("records")
    dataset = [make_conversation(x, include_hint=args.include_hint) for x in dataset]

    print(f"Initializing backend: {args.backend}...")

    llm_pipeline = create_llm_pipeline(backend=args.backend)

    # Logging directory
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_model_name = args.llm_model_id.replace("/", "_")
    log_dir = os.path.join("logs", f"{safe_model_name}_{now}")
    os.makedirs(log_dir, exist_ok=True)

    pass_at_n_count = 0
    majority_at_n_count = 0
    total = 0
    generation_counter = 0

    for idx, example in enumerate(dataset):
        prompt = example["prompt"]
        gold_flag = parse_flag_from_text(example["solution"])

        response = generate_text(
            llm_pipeline=llm_pipeline,
            backend=args.backend,
            model=args.llm_model_id,
            messages=prompt,
            temperature=args.temperature,
            top_p=args.top_p,
            max_new_tokens=args.max_new_tokens,
            n=args.n_samples,
        )

        generations = [choice.message.content for choice in response.choices]
        parsed_flags = [parse_flag_from_text(txt) for txt in generations]

        # Save each generation
        for i, txt in enumerate(generations):
            fname = f"{idx+1}_{i}_{parsed_flags[i] == gold_flag}.txt"
            with open(os.path.join(log_dir, fname), "w", encoding="utf-8") as f:
                f.write(txt)
                f.write("\nparsed_flag: " + str(parsed_flags[i]))

        # Metrics
        correct = [p == gold_flag for p in parsed_flags if p is not None]
        if any(correct):
            pass_at_n_count += 1
        if sum(correct) / args.n_samples > 0.5:
            majority_at_n_count += 1

        generation_counter += args.n_samples
        elapsed = time.time() - overall_start
        avg_per_gen = elapsed / generation_counter
        print(
            f"Question {idx+1}: {sum(correct)}/{args.n_samples} correct | "
            f"Elapsed: {elapsed:.2f}s | Avg/gen: {avg_per_gen:.2f}s"
        )

        total += 1


    total_elapsed = time.time() - overall_start
    pass_at_n = pass_at_n_count / total if total else 0
    majority_at_n = majority_at_n_count / total if total else 0

    print("=================================================")
    print(f"Total evaluated challenges : {total}")
    print(f"Total elapsed time         : {total_elapsed:.2f}s")
    print(f"Pass@{args.n_samples}                   : {pass_at_n:.3f}")
    print(f"Majority@{args.n_samples}               : {majority_at_n:.3f}")
    print(f"Average time per generation : {total_elapsed / generation_counter:.2f}s")
    print(f"Logs saved to               : {log_dir}")
    print("=================================================")


if __name__ == "__main__":
    main()
